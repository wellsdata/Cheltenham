---
title: "Cheltenham data processing"
author: "Wells"
date: "2025-11-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(janitor)
```
### Import extracted data
--data cleaning needs to remove a strange "" artifact and remove leading and trailing spaces
```{r}

master_index <- read_csv("MASTER_INDEX_11_3_2025.csv") |> 
  clean_names() |> 
  mutate(name = str_replace(name, "", ""),
         name = str_squish(name)) 
	

```

### Fix Names
--AI code to extract the name into first, middle and last.
--First extract the first name
--Second, examine the remaining - if there's a space, then populate a middle name, if not then just the last name

```{r}
master_index <- master_index |> 
  mutate(
    # Split name into parts
    name_parts = str_split(name, "\\s+"),
    
    # Extract first name (always the first element)
    first_name = map_chr(name_parts, ~ .x[1]),
    
    # Extract middle and last name
    middle_name = map_chr(name_parts, function(parts) {
      if (length(parts) > 2) {
        # Check if last part is a suffix
        last_part <- parts[length(parts)]
        is_suffix <- str_detect(last_part, "^(Jr\\.?|Sr\\.?|III?|IV|V)$")
        
        if (is_suffix && length(parts) > 3) {
          # Middle name is everything between first and last two parts
          paste(parts[2:(length(parts) - 2)], collapse = " ")
        } else if (!is_suffix) {
          # Middle name is everything between first and last part
          paste(parts[2:(length(parts) - 1)], collapse = " ")
        } else {
          NA_character_
        }
      } else {
        NA_character_
      }
    }),
    
    # Extract last name (includes suffix if present)
    last_name = map_chr(name_parts, function(parts) {
      last_part <- parts[length(parts)]
      is_suffix <- str_detect(last_part, "^(Jr\\.?|Sr\\.?|III?|IV|V)$")
      
      if (is_suffix && length(parts) >= 2) {
        # Include both last name and suffix
        paste(parts[(length(parts) - 1):length(parts)], collapse = " ")
      } else {
        last_part
      }
    })
  ) |> 
  select(-name_parts)


```


### Fix Birth Date
--a mix of years and full dates
--filter the years to one df, dates to another, process separately and join

```{r}
master_index <- master_index |> 
  mutate(
    birth_date_new = dmy(birth_date),
    # For entries where dmy() failed, try parsing as just a year
    birth_date_new = if_else(
      is.na(birth_date_new) & !is.na(birth_date),
      ymd(paste0(birth_date, "-01-01")),  # Set to January 1st of that year
      birth_date_new
    )
  ) |> 
   mutate(
    death_date_new = dmy(death_date),
    # For entries where dmy() failed, try parsing as just a year
   death_date_new = if_else(
      is.na(death_date_new) & !is.na(death_date),
      ymd(paste0(death_date, "-01-01")),  # Set to January 1st of that year
      death_date_new
    )
  )



```
### New age column
--divide days by 365

```{r}

master_index <- master_index |> 
  mutate(age_days = (death_date_new-birth_date_new)) |> 
   mutate(age = round(as.numeric(age_days) / 365.25))

```


#write to disk
```{r}

write_csv(master_index, "master_index_nov_4.csv")
```


### list of newspapers
```{r}
library(rvest)
library(tidyverse)

# List of LCCN codes
lccn_codes <- c("sn82006687", "sn83009667", "sn83016107", "sn83016209", 
                "sn83016348", "sn83016368", "sn83045462", "sn84026688",
                "sn85025350", "sn85025407", "sn85038292", "sn88065202",
                "sn88065721", "sn88065726", "sn89060060", "sn89060092",
                "sn89060124", "sn89060136")

# Function to scrape newspaper name from webpage
get_newspaper_name_scrape <- function(lccn) {
  url <- paste0("https://chroniclingamerica.loc.gov/lccn/", lccn, "/")
  
  tryCatch({
    page <- read_html(url)
    # Try to find the title
    title <- page %>% 
      html_element("title") %>% 
      html_text()
    
    # Clean up the title (usually format is "Newspaper Name | Chronicling America")
    name <- str_split(title, "\\|")[[1]][1] %>% str_trim()
    return(name)
  }, error = function(e) {
    message("Error for ", lccn, ": ", e$message)
    return(NA)
  })
  
  Sys.sleep(1)  # Be polite
}

# Get all newspaper names
newspaper_list <- tibble(
  lccn = lccn_codes
)

newspaper_list <- newspaper_list %>%
  mutate(newspaper_name = map_chr(lccn, get_newspaper_name_scrape))

print(newspaper_list)
```

```{r}
library(tidyverse)

# Create a dataframe with LCCN codes and the newspaper names we found
newspapers <- tibble(
  lccn = c("sn82006687", "sn83009667", "sn83016107", "sn83016209", 
           "sn83016348", "sn83016368", "sn83045462", "sn84026688",
           "sn85025350", "sn85025407", "sn85038292", "sn88065202",
           "sn88065721", "sn88065726", "sn89060060", "sn89060092",
           "sn89060124", "sn89060136"),
  
  newspaper_name = c(
    "Saint Mary's Beacon",                    # sn82006687
    "",                                        # sn83009667
    "",                                        # sn83016107
    "Montgomery County Sentinel",              # sn83016209
    "",                                        # sn83016348
    "",                                        # sn83016368
    "",                                        # sn83045462
    "",                                        # sn84026688
    "",                                        # sn85025350
    "Maryland Independent",                    # sn85025407
    "",                                        # sn85038292
    "",                                        # sn88065202
    "",                                        # sn88065721
    "Evening Capital and Maryland Gazette",    # sn88065726
    "",                                        # sn89060060
    "",                                        # sn89060092
    "",                                        # sn89060124
    ""                                         # sn89060136
  ),
  
  location = c(
    "Leonardtown, MD",                        # sn82006687
    "",                                        # sn83009667
    "",                                        # sn83016107
    "Rockville, MD",                          # sn83016209
    "",                                        # sn83016348
    "",                                        # sn83016368
    "",                                        # sn83045462
    "",                                        # sn84026688
    "",                                        # sn85025350
    "Port Tobacco/Waldorf, MD",               # sn85025407
    "",                                        # sn85038292
    "",                                        # sn88065202
    "",                                        # sn88065721
    "Annapolis, MD",                          # sn88065726
    "",                                        # sn89060060
    "",                                        # sn89060092
    "",                                        # sn89060124
    ""                                         # sn89060136
  )
)

# Print the table
print(newspapers)

# Save to CSV for easy editing
write_csv(newspapers, "maryland_newspapers.csv")

# To manually look up each one, you can visit:
# https://chroniclingamerica.loc.gov/lccn/[LCCN_CODE]/

# For example, to look up the first unknown one:
cat("\nTo look up newspapers, visit these URLs:\n")
for(i in 1:nrow(newspapers)) {
  if(newspapers$newspaper_name[i] == "") {
    cat(paste0("https://chroniclingamerica.loc.gov/lccn/", 
               newspapers$lccn[i], "/\n"))
  }
}

# After you manually fill in the CSV, you can read it back:
# newspapers_complete <- read_csv("maryland_newspapers.csv")
```

# Compile Long Form Data into a DF

```{r}
# Load required libraries
library(readr)
library(writexl)
library(stringr)

# Set the folder path
folder_path <- "/Users/gizmofo/Code/Cheltenham/input/scanned_long_form"

# Get list of all text files in the folder
text_files <- list.files(folder_path, pattern = "\\.txt$", full.names = TRUE)

# Create an empty list to store data frames
sheets_list <- list()

# Read each text file and add to the list
for (file in text_files) {
  # Get the file name without path and extension for the sheet name
  sheet_name <- tools::file_path_sans_ext(basename(file))
  
  # Truncate sheet name to 31 characters (Excel limit)
  sheet_name <- substr(sheet_name, 1, 31)
  
  # Read the text file
  content <- read_lines(file)
  
  # Split each line into Fields and Results
  fields <- character(length(content))
  results <- character(length(content))
  
  for (i in seq_along(content)) {
    line <- content[i]
    
    # Pattern: "NUMBER. FIELD TEXT,VALUE" or "NUMBER. (LETTER) FIELD TEXT,VALUE"
    # Look for the last comma as the separator
    if (grepl(",", line)) {
      # Find the last comma
      last_comma <- max(gregexpr(",", line)[[1]])
      fields[i] <- trimws(substr(line, 1, last_comma - 1))
      results[i] <- trimws(substr(line, last_comma + 1, nchar(line)))
    } else {
      # No comma found, entire line goes to Fields
      fields[i] <- trimws(line)
      results[i] <- ""
    }
  }
  
  # Convert to data frame with two columns
  df <- data.frame(
    Fields = fields,
    Results = results,
    stringsAsFactors = FALSE
  )
  
  # Add to list with sheet name
  sheets_list[[sheet_name]] <- df
}

# Write all sheets to Excel file
output_file <- file.path(folder_path, "long_form_compiled.xlsx")
write_xlsx(sheets_list, output_file)

cat("Excel file created successfully at:", output_file, "\n")
cat("Number of sheets:", length(sheets_list), "\n")
```




#old

```{r}
# Load required libraries
library(readr)
library(writexl)

# Set the folder path
folder_path <- "/Users/gizmofo/Code/Cheltenham/input/scanned_long_form"

# Get list of all text files in the folder
text_files <- list.files(folder_path, pattern = "\\.txt$", full.names = TRUE)

# Create an empty list to store data frames
sheets_list <- list()

# Read each text file and add to the list
for (file in text_files) {
  # Get the file name without path and extension for the sheet name
  sheet_name <- tools::file_path_sans_ext(basename(file))
  
  # Truncate sheet name to 31 characters (Excel limit)
  sheet_name <- substr(sheet_name, 1, 31)
  
  # Read the text file
  # If files have structured data (CSV-like), use read_delim or read_csv
  # For plain text, we'll read line by line
  content <- read_lines(file)
  
  # Convert to data frame
  df <- data.frame(Content = content, stringsAsFactors = FALSE)
  
  # Add to list with sheet name
  sheets_list[[sheet_name]] <- df
}

# Write all sheets to Excel file
#output_file <- "~/Desktop/long_form_compiled.xlsx"
output_file <- file.path(folder_path, "long_form_compiled.xlsx")
write_xlsx(sheets_list, output_file)

cat("Excel file created successfully at:", output_file, "\n")
cat("Number of sheets:", length(sheets_list), "\n")



```



# For short form
```{r}
# Load required libraries
library(readr)
library(writexl)

# Set the folder path
#folder_path <- "/Users/gizmofo/Code/data_journalism_class/Cheltenham/scanned_short_form"
#new file path
folder_path <- "/Users/gizmofo/Code/Cheltenham/input/scanned_short_form"
# Get list of all text files in the folder
text_files <- list.files(folder_path, pattern = "\\.txt$", full.names = TRUE)

# Create an empty list to store data frames
sheets_list <- list()

# Read each text file and add to the list
for (file in text_files) {
  # Get the file name without path and extension for the sheet name
  sheet_name <- tools::file_path_sans_ext(basename(file))
  
  # Truncate sheet name to 31 characters (Excel limit)
  sheet_name <- substr(sheet_name, 1, 31)
  
  # Read the text file
  content <- read_lines(file)
  
  # Split each line into fields and results by comma
  fields <- character()
  results <- character()
  
  for (line in content) {
    # Split by comma
    parts <- strsplit(line, ",", fixed = TRUE)[[1]]
    
    # Handle cases where there might be more than 2 parts
    if (length(parts) >= 2) {
      fields <- c(fields, trimws(parts[1]))
      # Join remaining parts in case there are commas in the result
      results <- c(results, trimws(paste(parts[-1], collapse = ",")))
    } else if (length(parts) == 1) {
      # If no comma, treat whole line as field with empty result
      fields <- c(fields, trimws(parts[1]))
      results <- c(results, "")
    }
  }
  
  # Convert to data frame with two columns
  df <- data.frame(
    Field = fields,
    Result = results,
    stringsAsFactors = FALSE
  )
  
  # Add to list with sheet name
  sheets_list[[sheet_name]] <- df
}

# Write all sheets to Excel file

output_file <- "~/Desktop/long_form_compiled.xlsx"
#output_file <- file.path(folder_path, "compiled_texts.xlsx")
write_xlsx(sheets_list, output_file)

cat("Excel file created successfully at:", output_file, "\n")
cat("Number of sheets:", length(sheets_list), "\n")
```



notes
https://claude.ai/share/4a95465e-7b00-4d8b-b71d-32fae4dfc9bf
